{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot Encoding the categorical output values to binary by adding 1's for that index and 0's otherwise\n",
    "def oneHotEncode(y):\n",
    "#     print('Before Encoding ')\n",
    "#     print(y)\n",
    "#     print(type(y))\n",
    "    enc = pd.get_dummies(y['y'])\n",
    "#     print('After Encoding ')\n",
    "#     print(enc)\n",
    "    return np.matrix(enc)\n",
    "\n",
    "# Applying Sigmoid Activation function to the hidden layer outputs used while forward propagation\n",
    "# works with scalar, arrays and matrix as well\n",
    "# Purpose of this method is to do squishing on the linear function\n",
    "def apply_sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "# Applying Sigmoid Activation function to the hidden layer outputs used while backward propagation to get gradients\n",
    "# works with scalar, arrays and matrix as well\n",
    "# Purpose of this method is to do undo the squishing on the linear function\n",
    "def apply_sigmoid_prime(z):\n",
    "    return np.dot(z, (1-z))\n",
    "\n",
    "\n",
    "# Forward propagation to calculate yHat by applying activation function twice\n",
    "def forward_propagate(X, W1, W2, b1, b2):\n",
    "    Z1 = np.dot(W1.T, X) + b1\n",
    "#     print('----Z1----')\n",
    "#     print(Z1)\n",
    "    A1 = apply_sigmoid(Z1)\n",
    "#     print('----A1----')\n",
    "#     print(A1)\n",
    "    Z2 = np.dot(W2.T, A1) + b2\n",
    "#     print('----Z2----')\n",
    "#     print(Z2)\n",
    "    A2 = apply_sigmoid(Z2) # Output of the last layer(output layer)\n",
    "#     print('----A2----')\n",
    "#     print(A2)\n",
    "    return A1, A2, Z1, Z2\n",
    "\n",
    "# Backward Propagation function to calculate the gradients\n",
    "def back_propagate(Z1, X, Y, A1, A2, W2):\n",
    "    m = X.shape[1]\n",
    "#     print(m)\n",
    "#     print('----Y----')\n",
    "#     print(Y)\n",
    "#     print('----A2----')\n",
    "#     print(A2)\n",
    "    dZ2 = (A2 - Y);\n",
    "#     print('----dz2----')\n",
    "#     print(dZ2)\n",
    "#     print(type(dZ2))\n",
    "#     print('----A1T----')\n",
    "#     print(A1.T)\n",
    "    dW2 = (1./m) * np.dot(dZ2, A1.T)\n",
    "#     print('----dw2----')\n",
    "#     print(dW2)\n",
    "    db2 = (1./m) * np.sum(dZ2, axis=1)\n",
    "    print('----db2----')\n",
    "    print(db2.shape)\n",
    "    dZ1 = (np.dot(dZ2, W2.T)* apply_sigmoid_prime(Z1).T) # element wise product of same dimension matrices\n",
    "#     print('----dz1----')\n",
    "#     print(dZ1)\n",
    "    dW1 = (1./m) * np.dot(dZ1, X.T)\n",
    "#     print('----dw1----')\n",
    "#     print(dW1)\n",
    "    db1 = (1./m) * np.sum(dZ1, axis =1)\n",
    "#     print('----db1----')\n",
    "#     print(db1)\n",
    "    return dW1, db1, dW2, db2\n",
    "    \n",
    "# Get the loss of for the training example\n",
    "def get_cost(Y, Yhat):\n",
    "    m= Y.shape[1]\n",
    "#     print(m)\n",
    "    loss = np.multiply(Y, np.log(Yhat)) + np.multiply((1-Y), np.log(1 - Yhat))\n",
    "#     print(loss)\n",
    "    cost = (-1/m) * np.sum(loss)\n",
    "    return cost\n",
    "\n",
    "def gradientDescent(X, Y, alpha, iters):  \n",
    "    # Call Forward propagation to calculate yHat\n",
    "    W1 = 0.01* np.random.randn(inputLayerSize,hiddenLayerSize);\n",
    "    W2 = 0.01* np.random.randn(hiddenLayerSize,outputLayerSize);\n",
    "    b1 = np.zeros((hiddenLayerSize,1));\n",
    "    b2 = np.zeros((outputLayerSize,1));\n",
    "    old_cost = sys.maxsize\n",
    "    new_cost = sys.maxsize\n",
    "    dW1 = None\n",
    "    db1 =None\n",
    "    dW2 = None\n",
    "    db2 = None\n",
    "    cost_history = []\n",
    "    for i in range(iters):\n",
    "        A1, A2, Z1, Z2 = forward_propagate(X, W1, W2, b1, b2)\n",
    "        dW1, db1, dW2, db2 = back_propagate(Z1, X, Y, A1, A2, W2)\n",
    "#         print('Before')\n",
    "#         print(W1)\n",
    "        W1 = W1 - (alpha * dW1).T\n",
    "#         print('After')\n",
    "#         print((alpha * db1).T)\n",
    "        b1 = b1 - (alpha * db1)\n",
    "        W2 = W2 - (alpha * dW2).T\n",
    "        b2 = b2 - (alpha * db2)\n",
    "        old_cost = new_cost\n",
    "        new_cost = get_cost(Y, A2)\n",
    "#         if(abs(old_cost - new_cost) < 0.00000000000001):\n",
    "#             print(\"breaking\" + str(old_cost) + str(new_cost))\n",
    "#             break;\n",
    "        print (\"cost : \" + str(new_cost) + \" Iteration: \" + str(i))\n",
    "        cost_history.append(new_cost)\n",
    "    return dW1 , db1, dW2, db2, cost_history, new_cost\n",
    "\n",
    "# Softmax activation function to get the probablity of the classes\n",
    "def softmax(z):\n",
    "    softMax = (np.exp(z).T / np.sum(np.exp(z),axis=1)).T\n",
    "    print('softmax is ')\n",
    "    print(softMax)\n",
    "    return softMax\n",
    "\n",
    "def plotCostHistory(cost_history):\n",
    "     plt.plot(cost_history)\n",
    "     plt.ylabel('Cost');\n",
    "     plt.xlabel('Iterations');\n",
    "     plt.title('Cost Progression with Iterations for different learning rates')\n",
    "     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Reading the training data\n",
    "data_train = pd.read_csv('ex3_train.csv', sep=\",\", encoding='utf-8', header='infer')\n",
    "df_train = data_train\n",
    "m = df_train.shape[0]\n",
    "\n",
    "y_train = pd.DataFrame(df_train['y'])\n",
    "X_train = df_train.drop(['y'], axis=1)\n",
    "\n",
    "X_train_mat = np.matrix(X_train).T\n",
    "y_train_mat = oneHotEncode(y_train).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 1500)\n",
      "(1, 1500)\n"
     ]
    }
   ],
   "source": [
    "# Reading the test data\n",
    "data_test = pd.read_csv('ex3_test.csv', sep=\",\", encoding='utf-8', header='infer')\n",
    "df_test = data_test\n",
    "\n",
    "y_test = df_test['y']\n",
    "X_test = df_test.drop(['y'], axis=1)\n",
    "\n",
    "X_test_mat = np.matrix(X_test).T\n",
    "y_test_mat = np.matrix(y_test)\n",
    "\n",
    "print(X_test_mat.shape)\n",
    "print(y_test_mat.shape)\n",
    "\n",
    "#Plot the selected pixel\n",
    "# num = 7\n",
    "# pixels = np.array(X_test[num:num+1], dtype='uint8')\n",
    "# print(y_test[num:num+1])\n",
    "# pixels = pixels.reshape((20, 20))\n",
    "# plt.imshow(pixels, cmap='gray')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# Adding one's column for bias\n",
    "# X_test.insert(0,-1,1) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n",
      "10\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "# Defining Hyperparameters\n",
    "inputLayerSize = X_train_mat.shape[0]\n",
    "hiddenLayerSize = 25 # As specified in assignment requirements\n",
    "outputLayerSize = 10\n",
    "print(inputLayerSize)\n",
    "print(outputLayerSize)\n",
    "print(hiddenLayerSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization function to check cost propagartion for different learning rates\n",
    "\n",
    "def optimize():\n",
    "    alpha = [0.1, 0.001]\n",
    "    for a in alpha:\n",
    "        dW1 , db1, dW2, db2, cost_history, new_cost = gradientDescent(X_train_mat, y_train_mat, a, 10)\n",
    "        plotCostHistory(cost_history)\n",
    "        return dW1 , db1, dW2, db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (10,3500) and (10,25) not aligned: 3500 (dim 1) != 10 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-4f576df864ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-1a3c741864a8>\u001b[0m in \u001b[0;36moptimize\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mdW1\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mdb1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradientDescent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_mat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_mat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mplotCostHistory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdW1\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mdb1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-fec55e9fe4d0>\u001b[0m in \u001b[0;36mgradientDescent\u001b[0;34m(X, Y, alpha, iters)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mA1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_propagate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mdW1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mback_propagate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;31m#         print('Before')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;31m#         print(W1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-fec55e9fe4d0>\u001b[0m in \u001b[0;36mback_propagate\u001b[0;34m(Z1, X, Y, A1, A2, W2)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;31m#     print('----db2----')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;31m#     print(db2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mdZ1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdZ2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0mapply_sigmoid_prime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# element wise product of same dimension matrices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;31m#     print('----dz1----')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;31m#     print(dZ1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (10,3500) and (10,25) not aligned: 3500 (dim 1) != 10 (dim 0)"
     ]
    }
   ],
   "source": [
    "optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "W1 = 0.01* np.random.randn(inputLayerSize,hiddenLayerSize);\n",
    "W2 = 0.01* np.random.randn(hiddenLayerSize,outputLayerSize);\n",
    "b1 = np.zeros((hiddenLayerSize,1));\n",
    "b2 = np.zeros((outputLayerSize,1));\n",
    "print(W1.shape)\n",
    "print(W2.shape)\n",
    "print(b1.shape)\n",
    "print(b2.shape)\n",
    "print(y_train_mat)\n",
    "A1, A2, Z1, Z2 = forward_propagate(X_train_mat, W1, W2, b1, b2)\n",
    "dW1, db1, dW2, db2 = back_propagate(Z1, X_train_mat, y_train_mat, A1, A2, W2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
