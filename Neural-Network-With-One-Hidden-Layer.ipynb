{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from termcolor import colored\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.(40pts) Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot Encoding the categorical output values to binary by adding 1's for that index and 0's otherwise\n",
    "def oneHotEncode(y):\n",
    "#     print('Before Encoding ')\n",
    "#     print(y)\n",
    "#     print(type(y))\n",
    "    enc = pd.get_dummies(y['y'])\n",
    "#     print('After Encoding ')\n",
    "#     print(enc)\n",
    "    return np.matrix(enc)\n",
    "\n",
    "# Applying Sigmoid Activation function to the hidden layer outputs used while forward propagation\n",
    "# works with scalar, arrays and matrix as well\n",
    "# Purpose of this method is to do squishing on the linear function\n",
    "def apply_sigmoid(z):\n",
    "    return 1./(1+np.exp(-z))\n",
    "\n",
    "# Applying Sigmoid Activation function to the hidden layer outputs used while backward propagation to get gradients\n",
    "# works with scalar, arrays and matrix as well\n",
    "# Purpose of this method is to do undo the squishing on the linear function\n",
    "def apply_sigmoid_prime(z):\n",
    "#     print('Before')\n",
    "#     print(z)\n",
    "    #inv = np.dot(z.T, (1-z))\n",
    "    inv = (np.exp(-z))/(np.power((1+np.exp(-z)),2))\n",
    "#     print('After')\n",
    "#     print(inv)\n",
    "    return inv\n",
    "\n",
    "\n",
    "# Forward propagation to calculate yHat by applying activation function twice\n",
    "def forward_propagate(X, W1, W2, b1, b2):\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "#     print('----Z1----')\n",
    "#     print(Z1)\n",
    "    A1 = apply_sigmoid(Z1)\n",
    "#     print('----A1----')\n",
    "#     print(A1)\n",
    "#     print('----W2----')\n",
    "#     print(W2)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "#     print('----Z2----')\n",
    "#     print(Z2)\n",
    "    A2 = apply_sigmoid(Z2) # Output of the last layer(output layer)\n",
    "#     print('----A2----')\n",
    "#     print(A2)\n",
    "    return A1, A2, Z1, Z2\n",
    "\n",
    "# Backward Propagation function to calculate the gradients\n",
    "def back_propagate(Z1, X, Y, A1, A2, W2):\n",
    "    m = X.shape[1]\n",
    "#     print(m)\n",
    "#     print('----Y----')\n",
    "#     print(Y)\n",
    "#     print('----A2----')\n",
    "#     print(A2)\n",
    "    dZ2 = A2 - Y\n",
    "#     print('----dz2----')\n",
    "#     print(dZ2)\n",
    "#     print(type(dZ2))\n",
    "#     print('----A1T----')\n",
    "#     print(A1.T)\n",
    "    dW2 = (1./m) * np.dot(dZ2, A1.T)\n",
    "#     print('----dw2----')\n",
    "#     print(dW2)\n",
    "    db2 = (1./m) * np.sum(dZ2, axis=1)\n",
    "#     print('----db2----')\n",
    "#     print(db2)\n",
    "    temp1 = np.dot( W2.T, dZ2 )\n",
    "    temp2 = apply_sigmoid_prime(Z1)\n",
    "#     print('----temp1----')\n",
    "#     print(temp1)\n",
    "#     print('----temp2----')\n",
    "#     print(temp2)\n",
    "    dZ1 = np.multiply(temp1 , temp2) # element wise product of same dimension matrices\n",
    "#     print('----dz1----')\n",
    "#     print(dZ1)\n",
    "    dW1 = (1./m) * np.dot(dZ1, X.T)\n",
    "#     print('----dw1----')\n",
    "#     print(dW1)\n",
    "    db1 = (1./m) * np.sum(dZ1, axis =1)\n",
    "#     print('----db1----')\n",
    "#     print(db1)\n",
    "    return dW1, db1, dW2, db2\n",
    "    \n",
    "# Get the loss of for the training example\n",
    "def get_cost(Y, Yhat):\n",
    "    m= Y.shape[1]\n",
    "#     print(m)\n",
    "    loss = np.multiply(np.log(Yhat),Y) + np.multiply((1.-Y), np.log(1. - Yhat))\n",
    "#     print(loss)\n",
    "    loss = np.sum(loss)\n",
    "    #print('loss is ' + str(loss))\n",
    "    cost = -1./m * np.sum(loss)\n",
    "    cost = np.squeeze(cost)\n",
    "    return cost\n",
    "\n",
    "def gradientDescent(X, Y, YOrg, alpha, iters):  \n",
    "    # Call Forward propagation to calculate yHat\n",
    "    W1 = np.random.randn(hiddenLayerSize, inputLayerSize) * 0.01;\n",
    "    W2 = np.random.randn(outputLayerSize, hiddenLayerSize) * 0.01;\n",
    "    b1 = np.zeros((hiddenLayerSize,1));\n",
    "    b2 = np.zeros((outputLayerSize,1));\n",
    "    old_cost = sys.maxsize\n",
    "    new_cost = sys.maxsize\n",
    "    dW1 = None\n",
    "    db1 = None\n",
    "    dW2 = None\n",
    "    db2 = None\n",
    "    cost_history = []\n",
    "    for i in range(iters):\n",
    "        A1, A2, Z1, Z2 = forward_propagate(X, W1, W2, b1, b2)\n",
    "        old_cost = new_cost\n",
    "        new_cost = get_cost(Y, A2)\n",
    "        dW1, db1, dW2, db2 = back_propagate(Z1, X, Y, A1, A2, W2)\n",
    "        W1 = W1 - alpha * dW1\n",
    "        b1 = b1 - alpha * db1\n",
    "        W2 = W2 - alpha * dW2\n",
    "        b2 = b2 - alpha * db2\n",
    "#         if(abs(old_cost - new_cost) < 0.00000000001):\n",
    "#             print(\"breaking\" + str(old_cost) + str(new_cost))\n",
    "#             break;\n",
    "        #print (\"cost : \" + str(new_cost) + \" Old cost : \" + str(old_cost) + \" Iteration: \" + str(i))\n",
    "        cost_history.append(new_cost)\n",
    "    A2 = softmax(A2)\n",
    "    accuracy1 = accuracy_score(YOrg, A2)\n",
    "    accuracy2 = get_accuracy(YOrg, A2)\n",
    "    return W1 , b1, W2, b2, cost_history, new_cost, accuracy1, accuracy2\n",
    "\n",
    "# Softmax activation function to get the probablity of the classes\n",
    "def softmax(z):\n",
    "    softMax = (np.exp(z) / np.sum(np.exp(z),axis=0))\n",
    "    softMax = np.matrix(np.argmax(softMax,axis=0)).T\n",
    "    return softMax\n",
    "\n",
    "def plotCostHistory(cost_history, alpha, i):\n",
    "     line = plt.plot(cost_history, label=alpha)\n",
    "     plt.ylabel('Cost');\n",
    "     plt.xlabel('Iterations');\n",
    "     plt.title('Cost Progression with Iterations for different learning rates')\n",
    "     plt.legend()\n",
    "        \n",
    "def get_accuracy(Y, Ypred):\n",
    "    Y = np.matrix(Y)\n",
    "    numcorrect = 0\n",
    "    for (x,y) in zip(Ypred,Y):\n",
    "        if(x[0]==y[0]):\n",
    "            numcorrect+=1\n",
    "    accuracy=numcorrect*100.0/len(Y)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. (5pts) Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Reading the training data\n",
    "data_train = pd.read_csv('ex3_train.csv', sep=\",\", encoding='utf-8', header='infer')\n",
    "df_train = data_train\n",
    "m = df_train.shape[0]\n",
    "\n",
    "y_train = pd.DataFrame(df_train['y'])\n",
    "X_train = df_train.drop(['y'], axis=1)\n",
    "\n",
    "X_train_mat = np.matrix(X_train).T\n",
    "y_train_mat = oneHotEncode(y_train).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reading the test data\n",
    "data_test = pd.read_csv('ex3_test.csv', sep=\",\", encoding='utf-8', header='infer')\n",
    "df_test = data_test\n",
    "\n",
    "y_test = pd.DataFrame(df_test['y'])\n",
    "X_test = df_test.drop(['y'], axis=1)\n",
    "\n",
    "X_test_mat = np.matrix(X_test).T\n",
    "y_test_mat = oneHotEncode(y_test).T\n",
    "\n",
    "# Plot the selected pixel\n",
    "# num = 7\n",
    "# pixels = np.array(X_test[num:num+1], dtype='uint8')\n",
    "# print(y_test[num:num+1])\n",
    "# pixels = pixels.reshape((20, 20))\n",
    "# plt.imshow(pixels, cmap='gray')\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n",
      "10\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "# Defining Hyperparameters\n",
    "inputLayerSize = X_train_mat.shape[0]\n",
    "hiddenLayerSize = 25 # As specified in assignment requirements\n",
    "outputLayerSize = 10\n",
    "print(inputLayerSize)\n",
    "print(outputLayerSize)\n",
    "print(hiddenLayerSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization function to check cost propagartion for different learning rates\n",
    "def optimize():\n",
    "    alpha = [1]\n",
    "    i=0\n",
    "    scores = pd.DataFrame(columns=['alpha','W1', 'b1', 'W2', 'b2','cost'])\n",
    "    print('*****************Training Data*********************')\n",
    "    for a in alpha:\n",
    "        W1 , b1, W2, b2, cost_history, new_cost, acc1, acc2 = gradientDescent(X_train_mat, y_train_mat, y_train, a, 15000)\n",
    "        scores.loc[i] = pd.Series({'alpha':a, 'cost':new_cost, 'W1' : W1, 'b1' : b1, 'W2' : W2, 'b2' : b2})\n",
    "        print(\"Cost with alpha \" + str(a) + \" is \" + colored(str(new_cost), 'green'))\n",
    "        print(\"Accuracy1 is \" + str(acc1) + \" Accuracy2 is \" + str(acc2))\n",
    "        plotCostHistory(cost_history, a, i)\n",
    "        i+=1\n",
    "    least_cost_comb = scores['cost'].idxmin()\n",
    "    alph = scores.iloc[[least_cost_comb]]['alpha'][least_cost_comb]\n",
    "    W1 = scores.iloc[[least_cost_comb]]['W1'][least_cost_comb]\n",
    "    b1 = scores.iloc[[least_cost_comb]]['b1'][least_cost_comb]\n",
    "    W2 = scores.iloc[[least_cost_comb]]['W2'][least_cost_comb]\n",
    "    b2 = scores.iloc[[least_cost_comb]]['b2'][least_cost_comb]\n",
    "    plt.show()\n",
    "    return W1 , b1, W2, b2, alph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************Training Data*********************\n",
      "Cost with alpha 1 is \u001b[32m0.0120701769021\u001b[0m\n",
      "Accuracy1 is 1.0 Accuracy2 is 100.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEWCAYAAACEz/viAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x103eaa5d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the best combination of weights and alpha\n",
    "W1 , b1, W2, b2, alph = optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************Test Data*********************\n",
      "Accuracy is 0.918666666667 91.8666666667\n"
     ]
    }
   ],
   "source": [
    "A1, A2, Z1, Z2 = forward_propagate(X_test_mat, W1, W2, b1, b2)\n",
    "A2 = softmax(A2)\n",
    "acc1 = accuracy_score(y_test, A2)\n",
    "acc2 = get_accuracy(y_test, A2)\n",
    "print('*****************Test Data*********************')\n",
    "print('Accuracy is ' + str(acc1) + ' ' + str(acc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(alph)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
