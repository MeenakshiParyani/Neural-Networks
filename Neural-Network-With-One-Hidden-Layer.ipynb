{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from termcolor import colored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.(40pts) Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot Encoding the categorical output values to binary by adding 1's for that index and 0's otherwise\n",
    "def oneHotEncode(y):\n",
    "    enc = pd.get_dummies(y['y'])\n",
    "    return np.matrix(enc)\n",
    "\n",
    "# Applying Sigmoid Activation function to the hidden layer outputs used while forward propagation\n",
    "# works with scalar, arrays and matrix as well\n",
    "# Purpose of this method is to do squishing on the linear function\n",
    "def apply_sigmoid(z):\n",
    "    return 1./(1+np.exp(-z))\n",
    "\n",
    "# Applying Sigmoid Activation function to the hidden layer outputs used while backward propagation to get gradients\n",
    "# works with scalar, arrays and matrix as well\n",
    "# Purpose of this method is to do undo the squishing on the linear function\n",
    "def apply_sigmoid_prime(z):\n",
    "    inv = (np.exp(-z))/(np.power((1+np.exp(-z)),2))\n",
    "    return inv\n",
    "\n",
    "\n",
    "# Forward propagation to calculate yHat by applying activation function twice\n",
    "def forward_propagate(X, W1, W2, b1, b2):\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = apply_sigmoid(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = apply_sigmoid(Z2) # Output of the last layer(output layer)\n",
    "    return A1, A2, Z1, Z2\n",
    "\n",
    "# Backward Propagation function to calculate the gradients\n",
    "def back_propagate(Z1, X, Y, A1, A2, W2):\n",
    "    m = X.shape[1]\n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = (1./m) * np.dot(dZ2, A1.T)\n",
    "    db2 = (1./m) * np.sum(dZ2, axis=1)\n",
    "    temp1 = np.dot( W2.T, dZ2 )\n",
    "    temp2 = apply_sigmoid_prime(Z1)\n",
    "    dZ1 = np.multiply(temp1 , temp2) # element wise product of same dimension matrices\n",
    "    dW1 = (1./m) * np.dot(dZ1, X.T)\n",
    "    db1 = (1./m) * np.sum(dZ1, axis =1)\n",
    "    return dW1, db1, dW2, db2\n",
    "    \n",
    "# Get the loss of for the training example\n",
    "def get_cost(Y, Yhat):\n",
    "    m= Y.shape[1]\n",
    "    loss = np.multiply(np.log(Yhat),Y) + np.multiply((1.-Y), np.log(1. - Yhat))\n",
    "    loss = np.sum(loss)\n",
    "    cost = -1./m * np.sum(loss)\n",
    "    cost = np.squeeze(cost)\n",
    "    return cost\n",
    "\n",
    "def gradientDescent(X, Y, YOrg, alpha, iters, hidden, inp, output):  \n",
    "    # Call Forward propagation to calculate yHat\n",
    "    W1, W2, b1, b2 = initialize_parameters(hidden, inp, output)\n",
    "    old_cost = sys.maxsize\n",
    "    new_cost = sys.maxsize\n",
    "    dW1 = None\n",
    "    db1 = None\n",
    "    dW2 = None\n",
    "    db2 = None\n",
    "    cost_history = []\n",
    "    for i in range(iters):\n",
    "        A1, A2, Z1, Z2 = forward_propagate(X, W1, W2, b1, b2)\n",
    "        old_cost = new_cost\n",
    "        new_cost = get_cost(Y, A2)\n",
    "        dW1, db1, dW2, db2 = back_propagate(Z1, X, Y, A1, A2, W2)\n",
    "        W1 = W1 - alpha * dW1\n",
    "        b1 = b1 - alpha * db1\n",
    "        W2 = W2 - alpha * dW2\n",
    "        b2 = b2 - alpha * db2\n",
    "        if(abs(old_cost - new_cost) < 0.00000000001):\n",
    "            print(\"breaking\" + str(old_cost) + str(new_cost))\n",
    "            break;\n",
    "        #print (\"cost : \" + str(new_cost) + \" Old cost : \" + str(old_cost) + \" Iteration: \" + str(i))\n",
    "        cost_history.append(new_cost)\n",
    "    A2 = softmax(A2)\n",
    "    accuracy = get_accuracy(YOrg, A2)\n",
    "    return W1 , b1, W2, b2, cost_history, new_cost, accuracy\n",
    "\n",
    "# Softmax activation function to get the probablity of the classes\n",
    "def softmax(z):\n",
    "    softMax = (np.exp(z) / np.sum(np.exp(z),axis=0))\n",
    "    softMax = np.matrix(np.argmax(softMax,axis=0)).T\n",
    "    return softMax\n",
    "\n",
    "def plotCostHistory(cost_history, alpha, i):\n",
    "     line = plt.plot(cost_history, label=alpha)\n",
    "     plt.ylabel('Cost');\n",
    "     plt.xlabel('Iterations');\n",
    "     plt.title('Cost Progression with Iterations for different learning rates')\n",
    "     plt.legend()\n",
    "        \n",
    "def get_accuracy(Y, Ypred):\n",
    "    Y = np.matrix(Y)\n",
    "    numcorrect = 0\n",
    "    for (x,y) in zip(Ypred,Y):\n",
    "        if(x[0]==y[0]):\n",
    "            numcorrect+=1\n",
    "    accuracy=numcorrect*100.0/len(Y)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. (5pts) Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Reading the training data\n",
    "data_train = pd.read_csv('ex3_train.csv', sep=\",\", encoding='utf-8', header='infer')\n",
    "df_train = data_train\n",
    "m = df_train.shape[0]\n",
    "\n",
    "y_train = pd.DataFrame(df_train['y'])\n",
    "X_train = df_train.drop(['y'], axis=1)\n",
    "\n",
    "X_train_mat = np.matrix(X_train).T\n",
    "y_train_mat = oneHotEncode(y_train).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     y\n",
      "200  6\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQgAAAD8CAYAAACLgjpEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10479a910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reading the test data\n",
    "data_test = pd.read_csv('ex3_test.csv', sep=\",\", encoding='utf-8', header='infer')\n",
    "df_test = data_test\n",
    "\n",
    "y_test = pd.DataFrame(df_test['y'])\n",
    "X_test = df_test.drop(['y'], axis=1)\n",
    "\n",
    "X_test_mat = np.matrix(X_test).T\n",
    "y_test_mat = oneHotEncode(y_test).T\n",
    "\n",
    "# Plot the selected pixel\n",
    "num = 200\n",
    "pixels = np.array(X_test[num:num+1], dtype='uint8')\n",
    "print(y_test[num:num+1])\n",
    "pixels = pixels.reshape((20, 20))\n",
    "plt.imshow(pixels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. (5pts) Initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights\n",
    "np.random.seed(1) # Setting random seed to 1\n",
    "def initialize_parameters(hiddenLayerSize, inputLayerSize, outputLayerSize):\n",
    "    W1 = np.random.randn(hiddenLayerSize, inputLayerSize) * 0.01;\n",
    "    W2 = np.random.randn(outputLayerSize, hiddenLayerSize) * 0.01;\n",
    "    b1 = np.zeros((hiddenLayerSize,1));\n",
    "    b2 = np.zeros((outputLayerSize,1));\n",
    "    return W1, W2, b1, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. (20pts) Neural Network model with 1 hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Layer Size is 400\n",
      "Hidden Layer Size is 25\n",
      "Output Layer Size is 10\n"
     ]
    }
   ],
   "source": [
    "# Defining Hyperparameters\n",
    "inputLayerSize = X_train_mat.shape[0]\n",
    "hiddenLayerSize = 25 # As specified in assignment requirements\n",
    "outputLayerSize = 10\n",
    "print('Input Layer Size is ' + str(inputLayerSize))\n",
    "print('Hidden Layer Size is ' + str(hiddenLayerSize))\n",
    "print('Output Layer Size is ' + str(outputLayerSize))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. (10pts) Predictions \n",
    "### 6. (20pts) Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization function to check cost propagartion for different learning rates\n",
    "def optimize():\n",
    "    alpha = [1]\n",
    "    i=0\n",
    "    scores = pd.DataFrame(columns=['alpha','W1', 'b1', 'W2', 'b2','accuracy'])\n",
    "    print(colored('*****************Training Data*********************', 'red'))\n",
    "    for a in alpha:\n",
    "        W1 , b1, W2, b2, cost_history, new_cost, acc = gradientDescent(X_train_mat, y_train_mat, \n",
    "                                                        y_train, a, 4504, hiddenLayerSize, inputLayerSize, outputLayerSize)\n",
    "        scores.loc[i] = pd.Series({'alpha':a, 'accuracy':acc, 'W1' : W1, 'b1' : b1, 'W2' : W2, 'b2' : b2})\n",
    "        print(\"Cost with \" + colored(\"Alpha \" + str(a),'green') + \" is \" + str(new_cost) + \" & \" + colored(\"Accuracy is \" + str(acc), 'green'))\n",
    "        plotCostHistory(cost_history, a, i)\n",
    "        i+=1\n",
    "    least_cost_comb = scores['accuracy'].idxmax()\n",
    "    alph = scores.iloc[[least_cost_comb]]['alpha'][least_cost_comb]\n",
    "    W1 = scores.iloc[[least_cost_comb]]['W1'][least_cost_comb]\n",
    "    b1 = scores.iloc[[least_cost_comb]]['b1'][least_cost_comb]\n",
    "    W2 = scores.iloc[[least_cost_comb]]['W2'][least_cost_comb]\n",
    "    b2 = scores.iloc[[least_cost_comb]]['b2'][least_cost_comb]\n",
    "    plt.show()\n",
    "    return W1 , b1, W2, b2, alph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m*****************Training Data*********************\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Get the best combination of weights and alpha\n",
    "W1 , b1, W2, b2, alph = optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "A1, A2, Z1, Z2 = forward_propagate(X_test_mat, W1, W2, b1, b2)\n",
    "A2 = softmax(A2)\n",
    "acc = get_accuracy(y_test, A2)\n",
    "print(colored('*****************Test Data************************', 'red'))\n",
    "print(colored('Test Data Optimized accuracy is ' + str(acc), 'green'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha is 1\n"
     ]
    }
   ],
   "source": [
    "print('Best alpha is ' + str(alph))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
